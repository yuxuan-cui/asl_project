{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef56cfbf-e984-4b16-b01e-bd1ecebce69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.12.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (1.21.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (0.15.1)\n",
      "Requirement already satisfied: packaging in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from datasets==2.12.0) (6.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from aiohttp->datasets==2.12.0) (4.6.3)\n",
      "Requirement already satisfied: filelock in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (3.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from packaging->datasets==2.12.0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.12.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.12.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from requests>=2.19.0->datasets==2.12.0) (2023.5.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from importlib-metadata->datasets==2.12.0) (3.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from pandas->datasets==2.12.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from pandas->datasets==2.12.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.12.0) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc117c79-5c27-4265-ac5f-60633f35fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63f92d-ac8e-4dca-b39c-3e1aaaac6e87",
   "metadata": {},
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a42243b-8723-4a3f-9ad3-12539d519430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 17:44:17.492992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:17.633435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:17.635191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:17.659187: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 17:44:17.660178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:17.661802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:17.663357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:26.853149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:26.855035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:26.856801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 17:44:26.875899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tfhub_handle_encoder=\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "tfhub_handle_preprocess=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260ffa6f-5fc3-49a7-9dbb-525af89c75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModelBert(keras.Model):\n",
    "    def __init__(self, \n",
    "                 num_tags,\n",
    "                 dropout_rate=0.1, \n",
    "                 tfhub_handle_encoder=\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "                 tfhub_handle_preprocess=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "        self.preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
    "        self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"BERT_encoder\")\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")                                                  \n",
    "    \n",
    "    def call(self, text_input):\n",
    "        encoder_inputs = self.preprocessing_layer(text_input)\n",
    "        outputs = self.encoder(encoder_inputs)\n",
    "        net = outputs[\"sequence_output\"]\n",
    "        net = self.dropout(net)\n",
    "        net = self.ff_final(net)\n",
    "        return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344755f-d69b-473f-bfd8-636872dd2db0",
   "metadata": {},
   "source": [
    "## Load the CoNLL 2003 dataset from the datasets library and process it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d732ba-7e25-49fe-98dc-ac080d73993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset conll2003 (/home/jupyter/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|██████████| 3/3 [00:00<00:00,  7.49it/s]\n"
     ]
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7239c2f-b14f-4b2f-8e5a-879ebcb9ee05",
   "metadata": {},
   "source": [
    "We will export this data to a tab-separated file format which will be easy to read as a\n",
    "`tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0465031c-ddd1-4c3e-b2d3-a934ab37254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            N = 128\n",
    "            ner_tags += ['-1'] * (N - len(ner_tags))\n",
    "            \n",
    "            tokens = record[\"tokens\"]\n",
    "            if len(tokens) > 0:\n",
    "                f.write(\n",
    "                    \" \".join(tokens)\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(map(str, ner_tags))\n",
    "                    + \"\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f1bb8e-4cbd-4102-b798-d90600577449",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"data1\")\n",
    "export_to_file(\"./data1/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data1/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cfa2c0b-b990-4230-8de9-3509f008d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(\"./data1/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data1/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540594d5-6316-423a-8bb5-564d403cdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'EU rejects German call to boycott British lamb .\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.take(1).as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e73521a-93a0-4fe9-a699-7939f38d8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ff4d4a-324b-4f5b-a5a0-11a8ed26bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)\n",
    "num_tags = len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74d59f-aef4-427d-b65c-2e55bf5f6f24",
   "metadata": {},
   "source": [
    "We will be using the following map function to transform the data in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d798f7d-f061-428f-a2db-7b1ac7458b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data_with_bert(record_input):\n",
    "    records = tf.strings.split(record_input, sep=\"\\t\")\n",
    "    input = records[0:1][0]\n",
    "    tags = records[1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int32)\n",
    "    tags += 1\n",
    "    return input, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bc0611-735d-4d2a-88d1-427a152c1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662ad603-efe7-43a8-a1ab-63a9571f47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data_with_bert)\n",
    "    .batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237cdd16-c48a-43d8-afdd-98fea7916d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data_with_bert)\n",
    "    .batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10977ddf-17cc-449b-8421-00deb61bbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_bert = NERModelBert(num_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d114891-df64-4a25-b154-5059aa8a604f",
   "metadata": {},
   "source": [
    "We will be using a custom loss function that will ignore the loss from padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ce1244ba-b223-4065-ae03-691bd266e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1314fc2a-4da7-42c2-9ceb-4d89a67a65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9d21b-224b-4e48-994d-4f4cf9dd57fe",
   "metadata": {},
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "00fd0124-4dd1-41e4-ae90-6870fbf87592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 92s 199ms/step - loss: 0.9702\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 88s 202ms/step - loss: 0.9019\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 89s 203ms/step - loss: 0.8926\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 89s 204ms/step - loss: 0.8921\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 90s 204ms/step - loss: 0.8902\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 90s 204ms/step - loss: 0.8890\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 90s 204ms/step - loss: 0.8878\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 90s 204ms/step - loss: 0.8877\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 89s 204ms/step - loss: 0.8872\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 89s 203ms/step - loss: 0.8871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60df5d7490>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model_bert.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model_bert.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c13fe-5116-4a4b-a909-286ae893bc59",
   "metadata": {},
   "source": [
    "Sample inference using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "631ff447-32d1-4da0-8cbf-0ce6ff62095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = [\"eu rejects german call to boycott british lamb\"]\n",
    "sample_output = [2,1,2,0,0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "53ab14a6-91ce-474b-877b-4e301c849686",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ner_model_bert.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3ad2999c-cf9a-4845-a897-a8b04d1bc6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0bec7ffd-4ea8-4e8b-8d76-b37c406be1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[5.8023623e-05, 6.4230782e-01, 5.8863591e-02, ...,\n",
       "         8.1768315e-03, 2.4920182e-02, 5.8613606e-03],\n",
       "        [4.3413165e-05, 7.6970142e-01, 4.0450260e-02, ...,\n",
       "         5.3748270e-03, 2.3849668e-02, 4.4001639e-03],\n",
       "        [3.4169167e-05, 8.3732164e-01, 2.9337905e-02, ...,\n",
       "         3.8941987e-03, 2.0997340e-02, 3.3079602e-03],\n",
       "        ...,\n",
       "        [3.4174427e-05, 8.3730841e-01, 2.9340724e-02, ...,\n",
       "         3.8945919e-03, 2.0996951e-02, 3.3082303e-03],\n",
       "        [3.4176028e-05, 8.3731008e-01, 2.9340776e-02, ...,\n",
       "         3.8945868e-03, 2.0996531e-02, 3.3083595e-03],\n",
       "        [3.4178611e-05, 8.3728075e-01, 2.9345147e-02, ...,\n",
       "         3.8951596e-03, 2.0998575e-02, 3.3087248e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1fd2af58-5f82-4108-ae45-19e36d392c5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_4762/2253603644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner_model_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# The default implementation does not use `x`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     return self.compiled_loss(\n\u001b[0;32m--> 919\u001b[0;31m         y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "ner_model_bert.compute_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab48eee-8f79-4f1e-aef7-411dd5c1aef0",
   "metadata": {},
   "source": [
    "## Metrics calculation\n",
    "\n",
    "Here is a function to calculate the metrics. The function calculates F1 score for the\n",
    "overall NER dataset as well as individual scores for each NER tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "077f4107-db6d-4006-a233-9188168e0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model_bert.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c722734-2444-48e7-8551-61eaf0c7f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 5295 phrases; correct: 3855.\n",
      "accuracy:  62.69%; (non-O)\n",
      "accuracy:  93.39%; precision:  72.80%; recall:  64.88%; FB1:  68.61\n",
      "              LOC: precision:  83.45%; recall:  79.86%; FB1:  81.61  1758\n",
      "             MISC: precision:  74.45%; recall:  65.73%; FB1:  69.82  814\n",
      "              ORG: precision:  65.34%; recall:  61.00%; FB1:  63.09  1252\n",
      "              PER: precision:  65.53%; recall:  52.33%; FB1:  58.19  1471\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c03cbd45-239e-4211-888c-b65f40718866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 103 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f786816bf80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "processed 51362 tokens with 5942 phrases; found: 5947 phrases; correct: 4056.\n",
      "accuracy:  65.99%; (non-O)\n",
      "accuracy:  93.18%; precision:  68.20%; recall:  68.26%; FB1:  68.23\n",
      "              LOC: precision:  74.17%; recall:  83.94%; FB1:  78.75  2079\n",
      "             MISC: precision:  75.56%; recall:  61.71%; FB1:  67.94  753\n",
      "              ORG: precision:  58.39%; recall:  61.74%; FB1:  60.02  1418\n",
      "              PER: precision:  65.82%; recall:  60.64%; FB1:  63.13  1697\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afca4a-2abc-49f4-943d-b7c9060fc863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7103ca9b-3157-400b-8680-506cf4f287b2",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this exercise, we created a simple transformer based named entity recognition model.\n",
    "We trained it on the CoNLL 2003 shared task data and got an overall F1 score of around 70%.\n",
    "State of the art NER models fine-tuned on pretrained models such as BERT or ELECTRA can easily\n",
    "get much higher F1 score -between 90-95% on this dataset owing to the inherent knowledge\n",
    "of words as part of the pretraining process and the usage of subword tokenization.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/ner-with-transformers)\n",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/ner_with_transformers).\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8514c3f3-77b8-4b0e-a900-f433eadd7a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(ner_model, to_file=\"a.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26ff9023-7629-4c5f-af7f-7e0e2377608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f774159e-e93b-4828-af84-5a407edcebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "56922908-e6d5-4d7c-be1f-37a0eaeebe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data_with_bert(record_input):\n",
    "    records = tf.strings.split(record_input, sep=\"\\t\")\n",
    "    input = records[0:1][0]\n",
    "    tags = records[1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.float32)\n",
    "    tags += 1\n",
    "    tags /=10\n",
    "    return input, tags\n",
    "\n",
    "def m2(inp1,inp2):\n",
    "    return bert_preprocess_model(inp1),inp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "790df97b-e817-40e6-b0d9-fd7d3ad73773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[[0.4 0.1 0.8 0.1 0.1 0.1 0.8 0.1 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. ]\n",
      " [0.2 0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. ]\n",
      " [0.6 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. ]\n",
      " [0.1 0.4 0.5 0.1 0.1 0.1 0.1 0.1 0.1 0.8 0.1 0.1 0.1 0.1 0.1 0.8 0.1 0.1\n",
      "  0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# for x,y in train_data.map(map_record_to_training_data_with_bert).batch(4).take(2).as_numpy_iterator():\n",
    "#     print(y)\n",
    "    \n",
    "for x,y in train_data.map(map_record_to_training_data_with_bert).batch(4).map(m2).take(1).as_numpy_iterator():\n",
    "    print(len(x['input_word_ids'][0]))\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592abe2-9b0f-4db9-a1c8-940a57524ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x  in train_data.map(map_record_to_training_data_with_bert).batch(1).take(2).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8efeaa3b-3968-4231-a139-0a93bfe180a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0'\n"
     ]
    }
   ],
   "source": [
    "for x  in train_data.take(1).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59a304af-4040-4474-8935-0f8fd30701fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.constant([[[b'a',b'b']]])\n",
    "split = tf.split(value, num_or_size_splits = value.shape[1], axis = 1)\n",
    "string = tf.strings.join(split,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "311cf14a-af69-4efb-ade7-64959056f283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 2), dtype=string, numpy=array([[[b'a', b'b']]], dtype=object)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00463e37-6760-44b8-aa72-d694251b0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "a = [1]\n",
    "b = map(lambda x, y: y if x is None else x, a, ['']*N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "faa4bf0e-87f7-4c57-85c4-c75187fff89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f64a788c4d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff65d4c7-5fa0-40bf-b03d-41a35ad9eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a += [''] * (N - len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4dfd6f1-db5a-43df-8b33-b917eef7484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, '', '', '', '']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1037965-1618-4d43-acbc-b7d53368e9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "bert_kernel",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "bert_kernel",
   "language": "python",
   "name": "bert_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
